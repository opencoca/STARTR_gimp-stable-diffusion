{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opencoca/gimp-stable-diffusion/blob/main/gimp-stable-diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU52ZvES6-1T"
      },
      "source": [
        "# GIMP Stable Diffusion v1.1.1 \n",
        "\n",
        "## Note: This Notebook takes 3-5 minutes to completly launch\n",
        "\n",
        "###**IMPORTANT: Please use [v1.1.0 or later of the GIMP plugin](https://raw.githubusercontent.com/opencoca/gimp-stable-diffusion/main/gimp-stable-diffusion.py)** \n",
        "###The plugin version can be found at the top of the plugin file \"gimp-stable-diffusion.py\" which is located in your GIMP plugins folder.\n",
        "\n",
        "\n",
        "Stable Diffusion notebook, which can be used together with the GIMP plugin to generate images. For prompt ideas checkout https://lexica.art/.\n",
        "\n",
        "Images are created with [Stable Diffusion](https://github.com/CompVis/stable-diffusion) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer and the [Stability.ai](https://stability.ai/) Team. [K Diffusion](https://github.com/crowsonkb/k-diffusion) by [Katherine Crowson](https://twitter.com/RiversHaveWings). You need to get the ckpt file and put it on your Google Drive first to use this. It can be downloaded from [HuggingFace](https://huggingface.co/CompVis/stable-diffusion).\n",
        "\n",
        "GIMP Server Notebook by [OpenCo.ca](https://github.com/opencoca), [deforum](https://discord.gg/upmXXsrwZc), and [BlueTurtleAI](https://twitter.com/BlueTurtleAI)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "DeqQ7pt1zdI7",
        "outputId": "9974ac5f-0496-47e0-8413-c85ec6d2eb2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Outputs will be saved to /content/gdrive/MyDrive/AI/StableDiffusion\n",
            "Models will be stored in /content/gdrive/MyDrive/AI/models/\n",
            "Local Path Variables:\n",
            "\n",
            "models_path: /content/gdrive/MyDrive/AI/models/\n",
            "output_path: /content/drive/MyDrive/AI/StableDiffusion\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive and Prepare Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/StableDiffusion\"\n",
        "models_path = \"/content/gdrive/MyDrive/AI/models/\"\n",
        "!mkdir -p $outputs_path\n",
        "!mkdir -p $models_path\n",
        "print(f\"Outputs will be saved to {outputs_path}\")\n",
        "print(f\"Models will be stored in {models_path}\")\n",
        "#@title Set Model Path\n",
        "import os\n",
        "\n",
        "# ask for the link\n",
        "print(\"Local Path Variables:\\n\")\n",
        "\n",
        "models_path = \"/content/models\"\n",
        "output_path = \"/content/output\"\n",
        "\n",
        "models_path_gdrive = \"/content/gdrive/MyDrive/AI/models/\" #@param {type:\"string\"}\n",
        "output_path_gdrive = \"/content/drive/MyDrive/AI/StableDiffusion\"\n",
        "models_path = models_path_gdrive\n",
        "output_path = output_path_gdrive\n",
        "\n",
        "#os.makedirs(models_path, exist_ok=True)\n",
        "#os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "print(f\"models_path: {models_path}\")\n",
        "print(f\"output_path: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "LJyLnGBaEcKM",
        "outputId": "816fb6ed-4000-4ce9-8571-f1886e278c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47bf134a6ca65af05f4958af9c25b6efd9b6739312d57ce77fcc4aa39477eb63\n"
          ]
        }
      ],
      "source": [
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}\n",
        "\n",
        "checkpoint_model_url = \"https://bafybeicrdgunwfjxm5yr7qqe5kgybaog65wnonymaeumzkto4eagrvwz2a.ipfs.dweb.link/stable-diffusion-v1.4-and-license.zip\"\n",
        "!(echo $checkpoint_model_file | sha256sum | cut -f1 -d' ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "81qmVZbrm2uu",
        "outputId": "b9f69606-b489-4f08-8791-f8ff3c07cfd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 40.1 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "numba 0.56.2 requires setuptools<60, but you have setuptools 65.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mReading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libtorrent-rasterbar9\n",
            "Suggested packages:\n",
            "  libtorrent-rasterbar-dbg\n",
            "The following NEW packages will be installed:\n",
            "  libtorrent-rasterbar9 python3-libtorrent\n",
            "0 upgraded, 2 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 1,569 kB of archives.\n",
            "After this operation, 5,718 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libtorrent-rasterbar9 amd64 1.1.5-1build1 [1,258 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-libtorrent amd64 1.1.5-1build1 [310 kB]\n",
            "Fetched 1,569 kB in 1s (1,665 kB/s)\n",
            "Selecting previously unselected package libtorrent-rasterbar9.\n",
            "(Reading database ... 159447 files and directories currently installed.)\n",
            "Preparing to unpack .../libtorrent-rasterbar9_1.1.5-1build1_amd64.deb ...\n",
            "Unpacking libtorrent-rasterbar9 (1.1.5-1build1) ...\n",
            "Selecting previously unselected package python3-libtorrent.\n",
            "Preparing to unpack .../python3-libtorrent_1.1.5-1build1_amd64.deb ...\n",
            "Unpacking python3-libtorrent (1.1.5-1build1) ...\n",
            "Setting up libtorrent-rasterbar9 (1.1.5-1build1) ...\n",
            "Setting up python3-libtorrent (1.1.5-1build1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\n",
            "Setting up environment...\n",
            "\n",
            "Environment set up in 59 seconds\n"
          ]
        }
      ],
      "source": [
        "#@title Setup Environment\n",
        "\n",
        "setup_environment = True #@param {type:\"boolean\"}\n",
        "print_subprocess = False #@param {type:\"boolean\"}\n",
        "\n",
        "!yes | python -m pip install --upgrade --quiet pip setuptools wheel && yes | python -m pip install lbry-libtorrent --quiet && yes | apt install python3-libtorrent --quiet\n",
        "\n",
        "if setup_environment:\n",
        "    import subprocess, time\n",
        "    print(\"Setting up environment...\")\n",
        "    start_time = time.time()\n",
        "    all_process = [\n",
        "        ['pip', 'install', 'torch==1.12.1+cu113', 'torchvision==0.13.1+cu113', '--extra-index-url', 'https://download.pytorch.org/whl/cu113'],\n",
        "        ['pip', 'install', 'omegaconf==2.2.3', 'einops==0.4.1', 'pytorch-lightning==1.7.4', 'torchmetrics==0.9.3', 'torchtext==0.13.1', 'transformers==4.21.2', 'kornia==0.6.7'],\n",
        "        ['git', 'clone', 'https://github.com/deforum/stable-diffusion'],\n",
        "        ['pip', 'install', '-e', 'git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers'],\n",
        "        ['pip', 'install', '-e', 'git+https://github.com/openai/CLIP.git@main#egg=clip'],\n",
        "        ['pip', 'install', 'accelerate', 'ftfy', 'jsonmerge', 'matplotlib', 'resize-right', 'timm', 'torchdiffeq'],\n",
        "        ['git', 'clone', 'https://github.com/shariqfarooq123/AdaBins.git'],\n",
        "        ['git', 'clone', 'https://github.com/isl-org/MiDaS.git'],\n",
        "        ['git', 'clone', 'https://github.com/MSFTserver/pytorch3d-lite.git'],\n",
        "        ['pip', 'install', 'flask-cloudflared']\n",
        "    ]\n",
        "    for process in all_process:\n",
        "        running = subprocess.run(process,stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if print_subprocess:\n",
        "            print(running)\n",
        "    \n",
        "    print(subprocess.run(['git', 'clone', 'https://github.com/deforum/k-diffusion/'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    with open('k-diffusion/k_diffusion/__init__.py', 'w') as f:\n",
        "        f.write('')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"Environment set up in {end_time-start_time:.0f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "81qmVZbrm4uu"
      },
      "outputs": [],
      "source": [
        "#@title Load Model\n",
        "\n",
        "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}\n",
        "\n",
        "checkpoint_model_url = \"https://ipfs.io/ipfs/bafybeicrdgunwfjxm5yr7qqe5kgybaog65wnonymaeumzkto4eagrvwz2a/stable-diffusion-v1.4-and-license.zip\"\n",
        "#@title Python Definitions\n",
        "import json\n",
        "from IPython import display\n",
        "\n",
        "import gc, math, os, pathlib, subprocess, sys, time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from skimage.exposure import match_histograms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from types import SimpleNamespace\n",
        "from torch import autocast\n",
        "\n",
        "sys.path.extend([\n",
        "    'src/taming-transformers',\n",
        "    'src/clip',\n",
        "    'stable-diffusion/',\n",
        "    'k-diffusion',\n",
        "    'pytorch3d-lite',\n",
        "    'AdaBins',\n",
        "    'MiDaS',\n",
        "])\n",
        "\n",
        "import py3d_tools as p3d\n",
        "\n",
        "from helpers import DepthModel, sampler_fn\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "def sanitize(prompt):\n",
        "    whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "    tmp = ''.join(filter(whitelist.__contains__, prompt))\n",
        "    return tmp.replace(' ', '_')\n",
        "\n",
        "def anim_frame_warp_2d(prev_img_cv2, args, anim_args, keys, frame_idx):\n",
        "    angle = keys.angle_series[frame_idx]\n",
        "    zoom = keys.zoom_series[frame_idx]\n",
        "    translation_x = keys.translation_x_series[frame_idx]\n",
        "    translation_y = keys.translation_y_series[frame_idx]\n",
        "\n",
        "    center = (args.W // 2, args.H // 2)\n",
        "    trans_mat = np.float32([[1, 0, translation_x], [0, 1, translation_y]])\n",
        "    rot_mat = cv2.getRotationMatrix2D(center, angle, zoom)\n",
        "    trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "    rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "    xform = np.matmul(rot_mat, trans_mat)\n",
        "\n",
        "    return cv2.warpPerspective(\n",
        "        prev_img_cv2,\n",
        "        xform,\n",
        "        (prev_img_cv2.shape[1], prev_img_cv2.shape[0]),\n",
        "        borderMode=cv2.BORDER_WRAP if anim_args.border == 'wrap' else cv2.BORDER_REPLICATE\n",
        "    )\n",
        "\n",
        "def anim_frame_warp_3d(prev_img_cv2, depth, anim_args, keys, frame_idx):\n",
        "    TRANSLATION_SCALE = 1.0/200.0 # matches Disco\n",
        "    translate_xyz = [\n",
        "        -keys.translation_x_series[frame_idx] * TRANSLATION_SCALE, \n",
        "        keys.translation_y_series[frame_idx] * TRANSLATION_SCALE, \n",
        "        -keys.translation_z_series[frame_idx] * TRANSLATION_SCALE\n",
        "    ]\n",
        "    rotate_xyz = [\n",
        "        math.radians(keys.rotation_3d_x_series[frame_idx]), \n",
        "        math.radians(keys.rotation_3d_y_series[frame_idx]), \n",
        "        math.radians(keys.rotation_3d_z_series[frame_idx])\n",
        "    ]\n",
        "    rot_mat = p3d.euler_angles_to_matrix(torch.tensor(rotate_xyz, device=device), \"XYZ\").unsqueeze(0)\n",
        "    result = transform_image_3d(prev_img_cv2, depth, rot_mat, translate_xyz, anim_args)\n",
        "    torch.cuda.empty_cache()\n",
        "    return result\n",
        "\n",
        "def add_noise(sample: torch.Tensor, noise_amt: float) -> torch.Tensor:\n",
        "    return sample + torch.randn(sample.shape, device=sample.device) * noise_amt\n",
        "\n",
        "def get_output_folder(output_path, batch_folder):\n",
        "    out_path = os.path.join(output_path,time.strftime('%Y-%m'))\n",
        "    if batch_folder != \"\":\n",
        "        out_path = os.path.join(out_path, batch_folder)\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "    return out_path\n",
        "\n",
        "def load_img(path, shape, use_alpha_as_mask=False):\n",
        "    # use_alpha_as_mask: Read the alpha channel of the image as the mask image\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw)\n",
        "    else:\n",
        "        image = Image.open(path)\n",
        "\n",
        "    if use_alpha_as_mask:\n",
        "        image = image.convert('RGBA')\n",
        "    else:\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    image = image.resize(shape, resample=Image.LANCZOS)\n",
        "\n",
        "    mask_image = None\n",
        "    if use_alpha_as_mask:\n",
        "      # Split alpha channel into a mask_image\n",
        "      red, green, blue, alpha = Image.Image.split(image)\n",
        "      mask_image = alpha.convert('L')\n",
        "      image = image.convert('RGB')\n",
        "\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    image = 2.*image - 1.\n",
        "\n",
        "    return image, mask_image\n",
        "\n",
        "def load_mask_latent(mask_input, shape):\n",
        "    # mask_input (str or PIL Image.Image): Path to the mask image or a PIL Image object\n",
        "    # shape (list-like len(4)): shape of the image to match, usually latent_image.shape\n",
        "    \n",
        "    if isinstance(mask_input, str): # mask input is probably a file name\n",
        "        if mask_input.startswith('http://') or mask_input.startswith('https://'):\n",
        "            mask_image = Image.open(requests.get(mask_input, stream=True).raw).convert('RGBA')\n",
        "        else:\n",
        "            mask_image = Image.open(mask_input).convert('RGBA')\n",
        "    elif isinstance(mask_input, Image.Image):\n",
        "        mask_image = mask_input\n",
        "    else:\n",
        "        raise Exception(\"mask_input must be a PIL image or a file name\")\n",
        "\n",
        "    mask_w_h = (shape[-1], shape[-2])\n",
        "    mask = mask_image.resize(mask_w_h, resample=Image.LANCZOS)\n",
        "    mask = mask.convert(\"L\")\n",
        "    return mask\n",
        "\n",
        "def prepare_mask(mask_input, mask_shape, mask_brightness_adjust=1.0, mask_contrast_adjust=1.0):\n",
        "    # mask_input (str or PIL Image.Image): Path to the mask image or a PIL Image object\n",
        "    # shape (list-like len(4)): shape of the image to match, usually latent_image.shape\n",
        "    # mask_brightness_adjust (non-negative float): amount to adjust brightness of the iamge, \n",
        "    #     0 is black, 1 is no adjustment, >1 is brighter\n",
        "    # mask_contrast_adjust (non-negative float): amount to adjust contrast of the image, \n",
        "    #     0 is a flat grey image, 1 is no adjustment, >1 is more contrast\n",
        "    \n",
        "    mask = load_mask_latent(mask_input, mask_shape)\n",
        "\n",
        "    # Mask brightness/contrast adjustments\n",
        "    if mask_brightness_adjust != 1:\n",
        "        mask = TF.adjust_brightness(mask, mask_brightness_adjust)\n",
        "    if mask_contrast_adjust != 1:\n",
        "        mask = TF.adjust_contrast(mask, mask_contrast_adjust)\n",
        "\n",
        "    # Mask image to array\n",
        "    mask = np.array(mask).astype(np.float32) / 255.0\n",
        "    mask = np.tile(mask,(4,1,1))\n",
        "    mask = np.expand_dims(mask,axis=0)\n",
        "    mask = torch.from_numpy(mask)\n",
        "\n",
        "    if args.invert_mask:\n",
        "        mask = ( (mask - 0.5) * -1) + 0.5\n",
        "    \n",
        "    mask = np.clip(mask,0,1)\n",
        "    return mask\n",
        "\n",
        "def maintain_colors(prev_img, color_match_sample, mode):\n",
        "    if mode == 'Match Frame 0 RGB':\n",
        "        return match_histograms(prev_img, color_match_sample, multichannel=True)\n",
        "    elif mode == 'Match Frame 0 HSV':\n",
        "        prev_img_hsv = cv2.cvtColor(prev_img, cv2.COLOR_RGB2HSV)\n",
        "        color_match_hsv = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2HSV)\n",
        "        matched_hsv = match_histograms(prev_img_hsv, color_match_hsv, multichannel=True)\n",
        "        return cv2.cvtColor(matched_hsv, cv2.COLOR_HSV2RGB)\n",
        "    else: # Match Frame 0 LAB\n",
        "        prev_img_lab = cv2.cvtColor(prev_img, cv2.COLOR_RGB2LAB)\n",
        "        color_match_lab = cv2.cvtColor(color_match_sample, cv2.COLOR_RGB2LAB)\n",
        "        matched_lab = match_histograms(prev_img_lab, color_match_lab, multichannel=True)\n",
        "        return cv2.cvtColor(matched_lab, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "\n",
        "def make_callback(sampler_name, dynamic_threshold=None, static_threshold=None, mask=None, init_latent=None, sigmas=None, sampler=None, masked_noise_modifier=1.0):  \n",
        "    # Creates the callback function to be passed into the samplers\n",
        "    # The callback function is applied to the image at each step\n",
        "    def dynamic_thresholding_(img, threshold):\n",
        "        # Dynamic thresholding from Imagen paper (May 2022)\n",
        "        s = np.percentile(np.abs(img.cpu()), threshold, axis=tuple(range(1,img.ndim)))\n",
        "        s = np.max(np.append(s,1.0))\n",
        "        torch.clamp_(img, -1*s, s)\n",
        "        torch.FloatTensor.div_(img, s)\n",
        "\n",
        "    # Callback for samplers in the k-diffusion repo, called thus:\n",
        "    #   callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n",
        "    def k_callback_(args_dict):\n",
        "        if dynamic_threshold is not None:\n",
        "            dynamic_thresholding_(args_dict['x'], dynamic_threshold)\n",
        "        if static_threshold is not None:\n",
        "            torch.clamp_(args_dict['x'], -1*static_threshold, static_threshold)\n",
        "        if mask is not None:\n",
        "            init_noise = init_latent + noise * args_dict['sigma']\n",
        "            is_masked = torch.logical_and(mask >= mask_schedule[args_dict['i']], mask != 0 )\n",
        "            new_img = init_noise * torch.where(is_masked,1,0) + args_dict['x'] * torch.where(is_masked,0,1)\n",
        "            args_dict['x'].copy_(new_img)\n",
        "\n",
        "    # Function that is called on the image (img) and step (i) at each step\n",
        "    def img_callback_(img, i):\n",
        "        # Thresholding functions\n",
        "        if dynamic_threshold is not None:\n",
        "            dynamic_thresholding_(img, dynamic_threshold)\n",
        "        if static_threshold is not None:\n",
        "            torch.clamp_(img, -1*static_threshold, static_threshold)\n",
        "        if mask is not None:\n",
        "            i_inv = len(sigmas) - i - 1\n",
        "            init_noise = sampler.stochastic_encode(init_latent, torch.tensor([i_inv]*batch_size).to(device), noise=noise)\n",
        "            is_masked = torch.logical_and(mask >= mask_schedule[i], mask != 0 )\n",
        "            new_img = init_noise * torch.where(is_masked,1,0) + img * torch.where(is_masked,0,1)\n",
        "            img.copy_(new_img)\n",
        "              \n",
        "    if init_latent is not None:\n",
        "        noise = torch.randn_like(init_latent, device=device) * masked_noise_modifier\n",
        "    if sigmas is not None and len(sigmas) > 0:\n",
        "        mask_schedule, _ = torch.sort(sigmas/torch.max(sigmas))\n",
        "    elif len(sigmas) == 0:\n",
        "        mask = None # no mask needed if no steps (usually happens because strength==1.0)\n",
        "    if sampler_name in [\"plms\",\"ddim\"]: \n",
        "        # Callback function formated for compvis latent diffusion samplers\n",
        "        if mask is not None:\n",
        "            assert sampler is not None, \"Callback function for stable-diffusion samplers requires sampler variable\"\n",
        "            batch_size = init_latent.shape[0]\n",
        "\n",
        "        callback = img_callback_\n",
        "    else: \n",
        "        # Default callback function uses k-diffusion sampler variables\n",
        "        callback = k_callback_\n",
        "\n",
        "    return callback\n",
        "\n",
        "def sample_from_cv2(sample: np.ndarray) -> torch.Tensor:\n",
        "    sample = ((sample.astype(float) / 255.0) * 2) - 1\n",
        "    sample = sample[None].transpose(0, 3, 1, 2).astype(np.float16)\n",
        "    sample = torch.from_numpy(sample)\n",
        "    return sample\n",
        "\n",
        "def sample_to_cv2(sample: torch.Tensor, type=np.uint8) -> np.ndarray:\n",
        "    sample_f32 = rearrange(sample.squeeze().cpu().numpy(), \"c h w -> h w c\").astype(np.float32)\n",
        "    sample_f32 = ((sample_f32 * 0.5) + 0.5).clip(0, 1)\n",
        "    sample_int8 = (sample_f32 * 255)\n",
        "    return sample_int8.astype(type)\n",
        "\n",
        "def transform_image_3d(prev_img_cv2, depth_tensor, rot_mat, translate, anim_args):\n",
        "    # adapted and optimized version of transform_image_3d from Disco Diffusion https://github.com/alembics/disco-diffusion \n",
        "    w, h = prev_img_cv2.shape[1], prev_img_cv2.shape[0]\n",
        "\n",
        "    aspect_ratio = float(w)/float(h)\n",
        "    near, far, fov_deg = anim_args.near_plane, anim_args.far_plane, anim_args.fov\n",
        "    persp_cam_old = p3d.FoVPerspectiveCameras(near, far, aspect_ratio, fov=fov_deg, degrees=True, device=device)\n",
        "    persp_cam_new = p3d.FoVPerspectiveCameras(near, far, aspect_ratio, fov=fov_deg, degrees=True, R=rot_mat, T=torch.tensor([translate]), device=device)\n",
        "\n",
        "    # range of [-1,1] is important to torch grid_sample's padding handling\n",
        "    y,x = torch.meshgrid(torch.linspace(-1.,1.,h,dtype=torch.float32,device=device),torch.linspace(-1.,1.,w,dtype=torch.float32,device=device))\n",
        "    z = torch.as_tensor(depth_tensor, dtype=torch.float32, device=device)\n",
        "    xyz_old_world = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1)\n",
        "\n",
        "    xyz_old_cam_xy = persp_cam_old.get_full_projection_transform().transform_points(xyz_old_world)[:,0:2]\n",
        "    xyz_new_cam_xy = persp_cam_new.get_full_projection_transform().transform_points(xyz_old_world)[:,0:2]\n",
        "\n",
        "    offset_xy = xyz_new_cam_xy - xyz_old_cam_xy\n",
        "    # affine_grid theta param expects a batch of 2D mats. Each is 2x3 to do rotation+translation.\n",
        "    identity_2d_batch = torch.tensor([[1.,0.,0.],[0.,1.,0.]], device=device).unsqueeze(0)\n",
        "    # coords_2d will have shape (N,H,W,2).. which is also what grid_sample needs.\n",
        "    coords_2d = torch.nn.functional.affine_grid(identity_2d_batch, [1,1,h,w], align_corners=False)\n",
        "    offset_coords_2d = coords_2d - torch.reshape(offset_xy, (h,w,2)).unsqueeze(0)\n",
        "\n",
        "    image_tensor = rearrange(torch.from_numpy(prev_img_cv2.astype(np.float32)), 'h w c -> c h w').to(device)\n",
        "    new_image = torch.nn.functional.grid_sample(\n",
        "        image_tensor.add(1/512 - 0.0001).unsqueeze(0), \n",
        "        offset_coords_2d, \n",
        "        mode=anim_args.sampling_mode, \n",
        "        padding_mode=anim_args.padding_mode, \n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    # convert back to cv2 style numpy array\n",
        "    result = rearrange(\n",
        "        new_image.squeeze().clamp(0,255), \n",
        "        'c h w -> h w c'\n",
        "    ).cpu().numpy().astype(prev_img_cv2.dtype)\n",
        "    return result\n",
        "\n",
        "def generate(args, return_latent=False, return_sample=False, return_c=False):\n",
        "    seed_everything(args.seed)\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "\n",
        "    sampler = PLMSSampler(model) if args.sampler == 'plms' else DDIMSampler(model)\n",
        "    model_wrap = CompVisDenoiser(model)\n",
        "    batch_size = args.n_samples\n",
        "    prompt = args.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    precision_scope = autocast if args.precision == \"autocast\" else nullcontext\n",
        "\n",
        "    init_latent = None\n",
        "    mask_image = None\n",
        "    init_image = None\n",
        "    if args.init_latent is not None:\n",
        "        init_latent = args.init_latent\n",
        "    elif args.init_sample is not None:\n",
        "        with precision_scope(\"cuda\"):\n",
        "            init_latent = model.get_first_stage_encoding(model.encode_first_stage(args.init_sample))\n",
        "    elif args.use_init and args.init_image != None and args.init_image != '':\n",
        "        init_image, mask_image = load_img(args.init_image, \n",
        "                                          shape=(args.W, args.H),  \n",
        "                                          use_alpha_as_mask=args.use_alpha_as_mask)\n",
        "        init_image = init_image.to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        with precision_scope(\"cuda\"):\n",
        "            init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space        \n",
        "\n",
        "    if not args.use_init and args.strength > 0 and args.strength_0_no_init:\n",
        "        print(\"\\nNo init image, but strength > 0. Strength has been auto set to 0, since use_init is False.\")\n",
        "        print(\"If you want to force strength > 0 with no init, please set strength_0_no_init to False.\\n\")\n",
        "        args.strength = 0\n",
        "\n",
        "    # Mask functions\n",
        "    if args.use_mask:\n",
        "        assert args.mask_file is not None or mask_image is not None, \"use_mask==True: An mask image is required for a mask. Please enter a mask_file or use an init image with an alpha channel\"\n",
        "        assert args.use_init, \"use_mask==True: use_init is required for a mask\"\n",
        "        assert init_latent is not None, \"use_mask==True: An latent init image is required for a mask\"\n",
        "\n",
        "        mask = prepare_mask(args.mask_file if mask_image is None else mask_image, \n",
        "                            init_latent.shape, \n",
        "                            args.mask_contrast_adjust, \n",
        "                            args.mask_brightness_adjust)\n",
        "        \n",
        "        if (torch.all(mask == 0) or torch.all(mask == 1)) and args.use_alpha_as_mask:\n",
        "            raise Warning(\"use_alpha_as_mask==True: Using the alpha channel from the init image as a mask, but the alpha channel is blank.\")\n",
        "        \n",
        "        mask = mask.to(device)\n",
        "        mask = repeat(mask, '1 ... -> b ...', b=batch_size)\n",
        "    else:\n",
        "        mask = None\n",
        "        \n",
        "    t_enc = int((1.0-args.strength) * args.steps)\n",
        "\n",
        "    # Noise schedule for the k-diffusion samplers (used for masking)\n",
        "    k_sigmas = model_wrap.get_sigmas(args.steps)\n",
        "    k_sigmas = k_sigmas[len(k_sigmas)-t_enc-1:]\n",
        "\n",
        "    if args.sampler in ['plms','ddim']:\n",
        "        sampler.make_schedule(ddim_num_steps=args.steps, ddim_eta=args.ddim_eta, ddim_discretize='fill', verbose=False)\n",
        "\n",
        "    callback = make_callback(sampler_name=args.sampler,\n",
        "                            dynamic_threshold=args.dynamic_threshold, \n",
        "                            static_threshold=args.static_threshold,\n",
        "                            mask=mask, \n",
        "                            init_latent=init_latent,\n",
        "                            sigmas=k_sigmas,\n",
        "                            sampler=sampler)    \n",
        "\n",
        "    results = []\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for prompts in data:\n",
        "                    uc = None\n",
        "                    if args.scale != 1.0:\n",
        "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "                    c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                    if args.init_c != None:\n",
        "                        c = args.init_c\n",
        "\n",
        "                    if args.sampler in [\"klms\",\"dpm2\",\"dpm2_ancestral\",\"heun\",\"euler\",\"euler_ancestral\"]:\n",
        "                        samples = sampler_fn(\n",
        "                            c=c, \n",
        "                            uc=uc, \n",
        "                            args=args, \n",
        "                            model_wrap=model_wrap, \n",
        "                            init_latent=init_latent, \n",
        "                            t_enc=t_enc, \n",
        "                            device=device, \n",
        "                            cb=callback)\n",
        "                    else:\n",
        "                        # args.sampler == 'plms' or args.sampler == 'ddim':\n",
        "                        if init_latent is not None and args.strength > 0:\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                        else:\n",
        "                            z_enc = torch.randn([args.n_samples, args.C, args.H // args.f, args.W // args.f], device=device)\n",
        "                        if args.sampler == 'ddim':\n",
        "                            samples = sampler.decode(z_enc, \n",
        "                                                     c, \n",
        "                                                     t_enc, \n",
        "                                                     unconditional_guidance_scale=args.scale,\n",
        "                                                     unconditional_conditioning=uc,\n",
        "                                                     img_callback=callback)\n",
        "                        elif args.sampler == 'plms': # no \"decode\" function in plms, so use \"sample\"\n",
        "                            shape = [args.C, args.H // args.f, args.W // args.f]\n",
        "                            samples, _ = sampler.sample(S=args.steps,\n",
        "                                                            conditioning=c,\n",
        "                                                            batch_size=args.n_samples,\n",
        "                                                            shape=shape,\n",
        "                                                            verbose=False,\n",
        "                                                            unconditional_guidance_scale=args.scale,\n",
        "                                                            unconditional_conditioning=uc,\n",
        "                                                            eta=args.ddim_eta,\n",
        "                                                            x_T=z_enc,\n",
        "                                                            img_callback=callback)\n",
        "                        else:\n",
        "                            raise Exception(f\"Sampler {args.sampler} not recognised.\")\n",
        "\n",
        "                    if return_latent:\n",
        "                        results.append(samples.clone())\n",
        "\n",
        "                    x_samples = model.decode_first_stage(samples)\n",
        "                    if return_sample:\n",
        "                        results.append(x_samples.clone())\n",
        "\n",
        "                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                    if return_c:\n",
        "                        results.append(c.clone())\n",
        "\n",
        "                    for x_sample in x_samples:\n",
        "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                        image = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                        results.append(image)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "CIUJ7lWI4v53",
        "outputId": "9a3ca797-7c67-4e08-ea1d-035b64c8c93a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461,
          "referenced_widgets": [
            "48888ae6fd874e7ca152c30cf0dd4281",
            "ae2096fedcb44959b5121064ca865210",
            "9c522976498944edae9e8ed0fff1b3a6",
            "7306525d9bb84ee1b0b01021b06c1935",
            "58d4427070bb4957b126e35ac10cae2b",
            "773a4df039c34245afbf2c777e1fe4fc",
            "447821eb64f2474b892574fde1c298d7",
            "a2e74cc886d14a579a61ab2dc9c3631a",
            "8a62c0791f524824ad787e4dd28d57e6",
            "259d8a9075fb4d7dae592e98443376fd",
            "dc9781e2145c4030898d562c99c166e0",
            "e988159374074046a4ced4c4dfc92dd2",
            "aeca48f1e8c54f2ab9fa909bb2dc5820",
            "8adc7cc5ebea4a5bb71613c0780b088d",
            "5993a7a94a0540a6b9927b9a2a596638",
            "955e07cb3e224cd088bf56b2ed64affe",
            "c372642afdab44bc86e5e1a94b1adb1e",
            "711f8c52a699495f8c0344ee6a4b0a7e",
            "b283050e72bb4c23bd85dd110e0bf05f",
            "62a925f05d5e465ab6454186b9d2276a",
            "e3f1913bf323475fab64aeb75de63d28",
            "a73eca5d806b45bbad87604153eb2c43",
            "b4d492a58cf94898bac9533d062feaf4",
            "ed04aca44dfc49f48fc7d8f57af4a6f6",
            "b71e4b8ea1204f9aa34a8b80063b3b83",
            "8de836530ff644efbaec1fb11957906a",
            "cd9cbb201fc2419b93c2cc5fe21c8912",
            "7b22b56f6c304d15b050b08501a592af",
            "fa3de8b0d016409ebb0f6bd5aacf3b2f",
            "7d3c127a5ef044c18c6958a49370a8b9",
            "6ecebade4e6d466eb7de5c27a2a167bd",
            "de5f41e700f14d828d9e01efd3550deb",
            "1b71e27651b044ae805d2ecf8d0c7459",
            "aaca4b507feb4450821a9affdc70f8ab",
            "08336d7dd4024d2ba724d52f45385e0e",
            "b7b5641f734841c3a79cfb8572d8a08e",
            "2a6c13bd2712478988e03d63b51c0965",
            "502161a4b0eb4d26ab46e443e942e188",
            "80b8e45abf1d4186b6f3f0f417d85728",
            "7f591e96c4ba4564999448bb6927da85",
            "918c967f779a4d00b6faf1779803a3e4",
            "173f2cd548994c4daa525409e6524a73",
            "a49b276d19574734b6374b1ff88a8ae2",
            "7e32771ff8a844c1864e7cade88d9c7f",
            "5950fce1b1554dbbbd6630d8519556a4",
            "46e438d2621141a8a55ca78ab08bc030",
            "23b52e171568407ebe1b1bf65d0cf977",
            "0cced359117d42778b6417cc4239b82a",
            "c635d208b6604aa1ba0ac8ef3b69cca6",
            "950da4583ca74f15909f94b32c74bffa",
            "7bb0c55f117140979c9de4e9019195fa",
            "1efe56b96bd44803a52a1f8eaa225ecf",
            "dfae274a0198472e9faaa25866c54e5d",
            "99cddeb75be6425b841ec608e1f47186",
            "163d190b56534299a6ab2022e49a1639",
            "c4378367578b45809acaab4679b5f8b3",
            "0a5f258a9cfa424db0f4bcda9c8b6a72",
            "c851e6474fd542d9b7f49ed77e3e8c01",
            "fe0c7c4aa7794dbea3049315edeedab2",
            "b2ef74b58ca548c5814fd560e7dbf4f0",
            "c39398d311de415d988c1fc8e5119bb2",
            "f40e45dc9b314b1bb5c1b8be388559b7",
            "dc12d530de354e14a53ac17cd923328b",
            "a1ce0ce09aac4066bbd63b707a0cb3d6",
            "0e2fb27a84244814896ebf22176404a5",
            "69cdb72771bb466ba7b35ca550df1ad0"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using config: ./stable-diffusion/configs/stable-diffusion/v1-inference.yaml\n",
            "/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt exists\n",
            "Using ckpt: /content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\n",
            "Loading model from /content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\n",
            "Global Step: 470000\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 859.52 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/939k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48888ae6fd874e7ca152c30cf0dd4281"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/512k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e988159374074046a4ced4c4dfc92dd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4d492a58cf94898bac9533d062feaf4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaca4b507feb4450821a9affdc70f8ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/4.41k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5950fce1b1554dbbbd6630d8519556a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4378367578b45809acaab4679b5f8b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'logit_scale', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias']\n",
            "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#@title Select and Load Model\n",
        "\n",
        "model_config = \"v1-inference.yaml\"\n",
        "model_checkpoint =  \"sd-v1-4.ckpt\" #@param [\"sd-v1-4.ckpt\"]\n",
        "custom_config_path = \"\"\n",
        "custom_checkpoint_path = \"\"\n",
        "\n",
        "load_on_run_all = True\n",
        "half_precision = True\n",
        "check_sha256 = False\n",
        "\n",
        "model_map = {\n",
        "    \"sd-v1-4-full-ema.ckpt\": {'sha256': '14749efc0ae8ef0329391ad4436feb781b402f4fece4883c7ad8d10556d8a36a'},\n",
        "    \"sd-v1-4.ckpt\": {'sha256': 'fe4efff1e174c627256e44ec2991ba279b3816e364b49f9be2abc0b3ff3f8556'},\n",
        "    \"sd-v1-3-full-ema.ckpt\": {'sha256': '54632c6e8a36eecae65e36cb0595fab314e1a1545a65209f24fde221a8d4b2ca'},\n",
        "    \"sd-v1-3.ckpt\": {'sha256': '2cff93af4dcc07c3e03110205988ff98481e86539c51a8098d4f2236e41f7f2f'},\n",
        "    \"sd-v1-2-full-ema.ckpt\": {'sha256': 'bc5086a904d7b9d13d2a7bccf38f089824755be7261c7399d92e555e1e9ac69a'},\n",
        "    \"sd-v1-2.ckpt\": {'sha256': '3b87d30facd5bafca1cbed71cfb86648aad75d1c264663c0cc78c7aea8daec0d'},\n",
        "    \"sd-v1-1-full-ema.ckpt\": {'sha256': 'efdeb5dc418a025d9a8cc0a8617e106c69044bc2925abecc8a254b2910d69829'},\n",
        "    \"sd-v1-1.ckpt\": {'sha256': '86cd1d3ccb044d7ba8db743d717c9bac603c4043508ad2571383f954390f3cea'}\n",
        "}\n",
        "\n",
        "# config path\n",
        "ckpt_config_path = custom_config_path if model_config == \"custom\" else os.path.join(models_path, model_config)\n",
        "if os.path.exists(ckpt_config_path):\n",
        "    print(f\"{ckpt_config_path} exists\")\n",
        "else:\n",
        "    ckpt_config_path = \"./stable-diffusion/configs/stable-diffusion/v1-inference.yaml\"\n",
        "print(f\"Using config: {ckpt_config_path}\")\n",
        "\n",
        "# checkpoint path or download\n",
        "ckpt_path = custom_checkpoint_path if model_checkpoint == \"custom\" else os.path.join(models_path, model_checkpoint)\n",
        "ckpt_valid = True\n",
        "if os.path.exists(ckpt_path):\n",
        "    print(f\"{ckpt_path} exists\")\n",
        "else:\n",
        "    print(f\"Please download model checkpoint and place in {os.path.join(models_path, model_checkpoint)}\")\n",
        "    ckpt_valid = False\n",
        "\n",
        "if check_sha256 and model_checkpoint != \"custom\" and ckpt_valid:\n",
        "    import hashlib\n",
        "    print(\"\\n...checking sha256\")\n",
        "    with open(ckpt_path, \"rb\") as f:\n",
        "        bytes = f.read() \n",
        "        hash = hashlib.sha256(bytes).hexdigest()\n",
        "        del bytes\n",
        "    if model_map[model_checkpoint][\"sha256\"] == hash:\n",
        "        print(\"hash is correct\\n\")\n",
        "    else:\n",
        "        print(\"hash in not correct\\n\")\n",
        "        ckpt_valid = False\n",
        "\n",
        "if ckpt_valid:\n",
        "    print(f\"Using ckpt: {ckpt_path}\")\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False, device='cuda', half_precision=True):\n",
        "    map_location = \"cuda\"\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=map_location)\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    if half_precision:\n",
        "        model = model.half().to(device)\n",
        "    else:\n",
        "        model = model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "if load_on_run_all and ckpt_valid:\n",
        "    local_config = OmegaConf.load(f\"{ckpt_config_path}\")\n",
        "    model = load_model_from_config(local_config, f\"{ckpt_path}\", half_precision=half_precision)\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qH74gBWDd2oq",
        "outputId": "7034c122-6d6a-4fe7-9369-f7085f6dba0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on https://pig-divided-options-ati.trycloudflare.com\n",
            " * Traffic stats available on http://127.0.0.1:8099/metrics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Sep/2022 15:56:11] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/Sep/2022 15:58:59] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "#@title Wait for GIMP requests at Backend URL\n",
        "#@markdown Please wait ~30 seconds before connecting to the Backend URL \n",
        "\n",
        "from io import BytesIO\n",
        "import json\n",
        "import base64\n",
        "from flask import Flask, Response, request, abort, make_response\n",
        "#from flask_ngrok import run_with_ngrok\n",
        "from flask_cloudflared import run_with_cloudflared\n",
        "\n",
        "prompts = []\n",
        "\n",
        "def DeforumArgs():\n",
        "    W = 0\n",
        "    H = 0\n",
        "\n",
        "    seed = -1\n",
        "    sampler = 'klms'\n",
        "    steps = 0\n",
        "    scale = 0\n",
        "    ddim_eta = 0.0\n",
        "    dynamic_threshold = None\n",
        "    static_threshold = None   \n",
        "\n",
        "    save_samples = False\n",
        "    save_settings = False\n",
        "    display_samples = False\n",
        "\n",
        "    n_batch = 1\n",
        "    batch_name = \"GIMP\"\n",
        "    filename_format = \"{timestring}_{index}_{prompt}.png\"\n",
        "    make_grid = False\n",
        "    grid_rows = 2 \n",
        "    outdir = get_output_folder(output_path, batch_name)\n",
        "\n",
        "    use_init = True\n",
        "    strength = 0\n",
        "    strength_0_no_init = True # Set the strength to 0 automatically when no init image is used\n",
        "    init_image = \"\"\n",
        "    # Whiter areas of the mask are areas that change more\n",
        "    use_mask = False\n",
        "    use_alpha_as_mask = False # use the alpha channel of the init image as the mask\n",
        "    mask_file = \"\"\n",
        "    invert_mask = False\n",
        "    # Adjust mask image, 1.0 is no adjustment. Should be positive numbers.\n",
        "    mask_brightness_adjust = 1.0\n",
        "    mask_contrast_adjust = 1.0\n",
        "\n",
        "    n_samples = 1 # doesnt do anything\n",
        "    precision = 'autocast' \n",
        "    C = 4\n",
        "    f = 8\n",
        "\n",
        "    prompt = \"\"\n",
        "    timestring = \"\"\n",
        "    init_latent = None\n",
        "    init_sample = None\n",
        "    init_c = None\n",
        "\n",
        "    return locals()\n",
        "\n",
        "def render_image_batch(args):\n",
        "    args.prompts = {k: f\"{v:05d}\" for v, k in enumerate(prompts)}\n",
        "    \n",
        "    # create output folder for the batch\n",
        "    #os.makedirs(args.outdir, exist_ok=True)\n",
        "    #if args.save_settings or args.save_samples:\n",
        "        #print(f\"Saving to {os.path.join(args.outdir, args.timestring)}_*\")\n",
        "\n",
        "    # save settings for the batch\n",
        "    #if args.save_settings:\n",
        "        #filename = os.path.join(args.outdir, f\"{args.timestring}_settings.txt\")\n",
        "        #with open(filename, \"w+\", encoding=\"utf-8\") as f:\n",
        "            #json.dump(dict(args.__dict__), f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    index = 0\n",
        "    \n",
        "    # function for init image batching\n",
        "    init_array = []\n",
        "    if args.use_init:\n",
        "        if args.init_image == \"\":\n",
        "            raise FileNotFoundError(\"No path was given for init_image\")\n",
        "        if args.init_image.startswith('http://') or args.init_image.startswith('https://'):\n",
        "            init_array.append(args.init_image)\n",
        "        elif not os.path.isfile(args.init_image):\n",
        "            if args.init_image[-1] != \"/\": # avoids path error by adding / to end if not there\n",
        "                args.init_image += \"/\" \n",
        "            for image in sorted(os.listdir(args.init_image)): # iterates dir and appends images to init_array\n",
        "                if image.split(\".\")[-1] in (\"png\", \"jpg\", \"jpeg\"):\n",
        "                    init_array.append(args.init_image + image)\n",
        "        else:\n",
        "            init_array.append(args.init_image)\n",
        "    else:\n",
        "        init_array = [\"\"]\n",
        "\n",
        "    # when doing large batches don't flood browser with images\n",
        "    clear_between_batches = args.n_batch >= 32\n",
        "\n",
        "    for iprompt, prompt in enumerate(prompts):  \n",
        "        args.prompt = prompt\n",
        "        print(f\"Prompt {iprompt+1} of {len(prompts)}\")\n",
        "        print(f\"{args.prompt}\")\n",
        "\n",
        "        all_images = []\n",
        "\n",
        "        for batch_index in range(args.n_batch):\n",
        "            if clear_between_batches and batch_index % 32 == 0: \n",
        "                display.clear_output(wait=True)            \n",
        "            print(f\"Batch {batch_index+1} of {args.n_batch}\")\n",
        "            \n",
        "            for image in init_array: # iterates the init images\n",
        "                args.init_image = image\n",
        "                results = generate(args)\n",
        "                for image in results:\n",
        "                    if args.make_grid:\n",
        "                        all_images.append(T.functional.pil_to_tensor(image))\n",
        "                    if args.save_samples:\n",
        "                        if args.filename_format == \"{timestring}_{index}_{prompt}.png\":\n",
        "                            filename = f\"{args.timestring}_{index:05}_{sanitize(prompt)[:160]}.png\"\n",
        "                        else:\n",
        "                            filename = f\"{args.timestring}_{index:05}_{args.seed}.png\"\n",
        "                        image.save(os.path.join(args.outdir, filename))\n",
        "                    if args.display_samples:\n",
        "                        display.display(image)\n",
        "                    index += 1\n",
        "                #args.seed = next_seed(args)\n",
        "\n",
        "        #print(len(all_images))\n",
        "        if args.make_grid:\n",
        "            grid = make_grid(all_images, nrow=int(len(all_images)/args.grid_rows))\n",
        "            grid = rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "            filename = f\"{args.timestring}_{iprompt:05d}_grid_{args.seed}.png\"\n",
        "            grid_image = Image.fromarray(grid.astype(np.uint8))\n",
        "            grid_image.save(os.path.join(args.outdir, filename))\n",
        "            display.clear_output(wait=True)            \n",
        "            display.display(grid_image)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "args = SimpleNamespace(**DeforumArgs())\n",
        "args.timestring = time.strftime('%Y%m%d%H%M%S')\n",
        "\n",
        "API_VERSION = 4\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/api/img2img\", methods=[\"POST\"])\n",
        "def img2img():\n",
        "    r = request\n",
        "    data = r.data.decode(\"utf-8\")\n",
        "    data = json.loads(data)\n",
        "\n",
        "    api_version = 0\n",
        "\n",
        "    if \"api_version\" in data:\n",
        "       api_version = int(data[\"api_version\"])\n",
        "\n",
        "    if api_version != API_VERSION:\n",
        "       abort(405)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"Parameters sent from Gimp\")\n",
        "    print(\"inpainting: \" + str(data[\"inpainting\"]) + \", mask_brightness: \" + str(data[\"mask_brightness\"]) + \", mask_contrast: \" + str(data[\"mask_contrast\"]) + \", init_strength: \" + str(data[\"init_strength\"]) + \", prompt_strength: \" + str(data[\"prompt_strength\"]) + \", steps: \" + str(data[\"steps\"]) + \", width: \" + str(data[\"width\"]) + \", height: \" + str(data[\"height\"]) + \", prompt: \" + data[\"prompt\"] + \", seed: \" + str(data[\"seed\"]) + \", api_version: \" + str(data[\"api_version\"]))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    init_img = os.path.join(output_path, \"init.png\")\n",
        "    img_data = base64.b64decode(data[\"init_img\"])\n",
        "    img_file = open(init_img, \"wb+\")\n",
        "    img_file.write(img_data)\n",
        "    img_file.close()\n",
        "    args.init_image = init_img\n",
        "\n",
        "    args.W, args.H = map(lambda x: x - x % 64, (int(data[\"width\"]), int(data[\"height\"])))\n",
        "    args.strength = max(0.0, min(1.0, float(data[\"init_strength\"])))\n",
        "    args.scale = float(data[\"prompt_strength\"])\n",
        "    args.steps = int(data[\"steps\"])\n",
        "    args.use_mask = False\n",
        "    args.mask_file = \"\"\n",
        "\n",
        "    if bool(data[\"inpainting\"]) == True:\n",
        "       args.strength = 0.0\n",
        "       args.use_mask = True\n",
        "       args.mask_file = init_img\n",
        "       args.mask_brightness_adjust = float(data[\"mask_brightness\"])\n",
        "       args.mask_contrast_adjust = float(data[\"mask_contrast\"])\n",
        "\n",
        "    global prompts\n",
        "    prompts = [data[\"prompt\"]]\n",
        "\n",
        "    print(\"Parameters used for generating\")\n",
        "    print(args)\n",
        "\n",
        "    imgs_return = []\n",
        "\n",
        "    for counter in range(data[\"image_count\"]):\n",
        "       # clean up unused memory\n",
        "       gc.collect()\n",
        "       torch.cuda.empty_cache()\n",
        "    \n",
        "       args.seed = int(data[\"seed\"]) if int(data[\"seed\"]) != -1 else random.randint(0, 2**32)\n",
        "\n",
        "       img = render_image_batch(args)[0]\n",
        "\n",
        "       img_data = BytesIO()\n",
        "       img.save(img_data, format=\"PNG\")\n",
        "       img_data.seek(0)\n",
        "       img_encoded = base64.b64encode(img_data.read())\n",
        "       img_encoded = img_encoded.decode(\"utf-8\")\n",
        "\n",
        "       img_return = {\"seed\": args.seed, \"image\": img_encoded}\n",
        "       imgs_return.append(img_return)\n",
        "\n",
        "    data_return = {\"images\": imgs_return}\n",
        "    data_return = json.dumps(data_return)\n",
        "\n",
        "    if os.path.exists(init_img):\n",
        "       os.remove(init_img)\n",
        "\n",
        "    response = make_response()\n",
        "    response.headers[\"mimetype\"] = \"application/json\"\n",
        "    response.data = data_return\n",
        "    return response\n",
        "\n",
        "run_with_cloudflared(app)\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48888ae6fd874e7ca152c30cf0dd4281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae2096fedcb44959b5121064ca865210",
              "IPY_MODEL_9c522976498944edae9e8ed0fff1b3a6",
              "IPY_MODEL_7306525d9bb84ee1b0b01021b06c1935"
            ],
            "layout": "IPY_MODEL_58d4427070bb4957b126e35ac10cae2b"
          }
        },
        "ae2096fedcb44959b5121064ca865210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_773a4df039c34245afbf2c777e1fe4fc",
            "placeholder": "​",
            "style": "IPY_MODEL_447821eb64f2474b892574fde1c298d7",
            "value": "Downloading vocab.json: 100%"
          }
        },
        "9c522976498944edae9e8ed0fff1b3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2e74cc886d14a579a61ab2dc9c3631a",
            "max": 961143,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a62c0791f524824ad787e4dd28d57e6",
            "value": 961143
          }
        },
        "7306525d9bb84ee1b0b01021b06c1935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259d8a9075fb4d7dae592e98443376fd",
            "placeholder": "​",
            "style": "IPY_MODEL_dc9781e2145c4030898d562c99c166e0",
            "value": " 939k/939k [00:00&lt;00:00, 3.25MB/s]"
          }
        },
        "58d4427070bb4957b126e35ac10cae2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773a4df039c34245afbf2c777e1fe4fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "447821eb64f2474b892574fde1c298d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2e74cc886d14a579a61ab2dc9c3631a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a62c0791f524824ad787e4dd28d57e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "259d8a9075fb4d7dae592e98443376fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc9781e2145c4030898d562c99c166e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e988159374074046a4ced4c4dfc92dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aeca48f1e8c54f2ab9fa909bb2dc5820",
              "IPY_MODEL_8adc7cc5ebea4a5bb71613c0780b088d",
              "IPY_MODEL_5993a7a94a0540a6b9927b9a2a596638"
            ],
            "layout": "IPY_MODEL_955e07cb3e224cd088bf56b2ed64affe"
          }
        },
        "aeca48f1e8c54f2ab9fa909bb2dc5820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c372642afdab44bc86e5e1a94b1adb1e",
            "placeholder": "​",
            "style": "IPY_MODEL_711f8c52a699495f8c0344ee6a4b0a7e",
            "value": "Downloading merges.txt: 100%"
          }
        },
        "8adc7cc5ebea4a5bb71613c0780b088d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b283050e72bb4c23bd85dd110e0bf05f",
            "max": 524619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62a925f05d5e465ab6454186b9d2276a",
            "value": 524619
          }
        },
        "5993a7a94a0540a6b9927b9a2a596638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f1913bf323475fab64aeb75de63d28",
            "placeholder": "​",
            "style": "IPY_MODEL_a73eca5d806b45bbad87604153eb2c43",
            "value": " 512k/512k [00:00&lt;00:00, 2.04MB/s]"
          }
        },
        "955e07cb3e224cd088bf56b2ed64affe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c372642afdab44bc86e5e1a94b1adb1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711f8c52a699495f8c0344ee6a4b0a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b283050e72bb4c23bd85dd110e0bf05f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62a925f05d5e465ab6454186b9d2276a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3f1913bf323475fab64aeb75de63d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a73eca5d806b45bbad87604153eb2c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4d492a58cf94898bac9533d062feaf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed04aca44dfc49f48fc7d8f57af4a6f6",
              "IPY_MODEL_b71e4b8ea1204f9aa34a8b80063b3b83",
              "IPY_MODEL_8de836530ff644efbaec1fb11957906a"
            ],
            "layout": "IPY_MODEL_cd9cbb201fc2419b93c2cc5fe21c8912"
          }
        },
        "ed04aca44dfc49f48fc7d8f57af4a6f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b22b56f6c304d15b050b08501a592af",
            "placeholder": "​",
            "style": "IPY_MODEL_fa3de8b0d016409ebb0f6bd5aacf3b2f",
            "value": "Downloading special_tokens_map.json: 100%"
          }
        },
        "b71e4b8ea1204f9aa34a8b80063b3b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d3c127a5ef044c18c6958a49370a8b9",
            "max": 389,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ecebade4e6d466eb7de5c27a2a167bd",
            "value": 389
          }
        },
        "8de836530ff644efbaec1fb11957906a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de5f41e700f14d828d9e01efd3550deb",
            "placeholder": "​",
            "style": "IPY_MODEL_1b71e27651b044ae805d2ecf8d0c7459",
            "value": " 389/389 [00:00&lt;00:00, 9.33kB/s]"
          }
        },
        "cd9cbb201fc2419b93c2cc5fe21c8912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b22b56f6c304d15b050b08501a592af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa3de8b0d016409ebb0f6bd5aacf3b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d3c127a5ef044c18c6958a49370a8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ecebade4e6d466eb7de5c27a2a167bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de5f41e700f14d828d9e01efd3550deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b71e27651b044ae805d2ecf8d0c7459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aaca4b507feb4450821a9affdc70f8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08336d7dd4024d2ba724d52f45385e0e",
              "IPY_MODEL_b7b5641f734841c3a79cfb8572d8a08e",
              "IPY_MODEL_2a6c13bd2712478988e03d63b51c0965"
            ],
            "layout": "IPY_MODEL_502161a4b0eb4d26ab46e443e942e188"
          }
        },
        "08336d7dd4024d2ba724d52f45385e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b8e45abf1d4186b6f3f0f417d85728",
            "placeholder": "​",
            "style": "IPY_MODEL_7f591e96c4ba4564999448bb6927da85",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "b7b5641f734841c3a79cfb8572d8a08e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_918c967f779a4d00b6faf1779803a3e4",
            "max": 905,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_173f2cd548994c4daa525409e6524a73",
            "value": 905
          }
        },
        "2a6c13bd2712478988e03d63b51c0965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a49b276d19574734b6374b1ff88a8ae2",
            "placeholder": "​",
            "style": "IPY_MODEL_7e32771ff8a844c1864e7cade88d9c7f",
            "value": " 905/905 [00:00&lt;00:00, 26.1kB/s]"
          }
        },
        "502161a4b0eb4d26ab46e443e942e188": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b8e45abf1d4186b6f3f0f417d85728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f591e96c4ba4564999448bb6927da85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "918c967f779a4d00b6faf1779803a3e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "173f2cd548994c4daa525409e6524a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a49b276d19574734b6374b1ff88a8ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e32771ff8a844c1864e7cade88d9c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5950fce1b1554dbbbd6630d8519556a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46e438d2621141a8a55ca78ab08bc030",
              "IPY_MODEL_23b52e171568407ebe1b1bf65d0cf977",
              "IPY_MODEL_0cced359117d42778b6417cc4239b82a"
            ],
            "layout": "IPY_MODEL_c635d208b6604aa1ba0ac8ef3b69cca6"
          }
        },
        "46e438d2621141a8a55ca78ab08bc030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_950da4583ca74f15909f94b32c74bffa",
            "placeholder": "​",
            "style": "IPY_MODEL_7bb0c55f117140979c9de4e9019195fa",
            "value": "Downloading config.json: 100%"
          }
        },
        "23b52e171568407ebe1b1bf65d0cf977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1efe56b96bd44803a52a1f8eaa225ecf",
            "max": 4519,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfae274a0198472e9faaa25866c54e5d",
            "value": 4519
          }
        },
        "0cced359117d42778b6417cc4239b82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99cddeb75be6425b841ec608e1f47186",
            "placeholder": "​",
            "style": "IPY_MODEL_163d190b56534299a6ab2022e49a1639",
            "value": " 4.41k/4.41k [00:00&lt;00:00, 130kB/s]"
          }
        },
        "c635d208b6604aa1ba0ac8ef3b69cca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "950da4583ca74f15909f94b32c74bffa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb0c55f117140979c9de4e9019195fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1efe56b96bd44803a52a1f8eaa225ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfae274a0198472e9faaa25866c54e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99cddeb75be6425b841ec608e1f47186": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "163d190b56534299a6ab2022e49a1639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4378367578b45809acaab4679b5f8b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a5f258a9cfa424db0f4bcda9c8b6a72",
              "IPY_MODEL_c851e6474fd542d9b7f49ed77e3e8c01",
              "IPY_MODEL_fe0c7c4aa7794dbea3049315edeedab2"
            ],
            "layout": "IPY_MODEL_b2ef74b58ca548c5814fd560e7dbf4f0"
          }
        },
        "0a5f258a9cfa424db0f4bcda9c8b6a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c39398d311de415d988c1fc8e5119bb2",
            "placeholder": "​",
            "style": "IPY_MODEL_f40e45dc9b314b1bb5c1b8be388559b7",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "c851e6474fd542d9b7f49ed77e3e8c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc12d530de354e14a53ac17cd923328b",
            "max": 1710671599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1ce0ce09aac4066bbd63b707a0cb3d6",
            "value": 1710671599
          }
        },
        "fe0c7c4aa7794dbea3049315edeedab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e2fb27a84244814896ebf22176404a5",
            "placeholder": "​",
            "style": "IPY_MODEL_69cdb72771bb466ba7b35ca550df1ad0",
            "value": " 1.59G/1.59G [00:41&lt;00:00, 42.4MB/s]"
          }
        },
        "b2ef74b58ca548c5814fd560e7dbf4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c39398d311de415d988c1fc8e5119bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f40e45dc9b314b1bb5c1b8be388559b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc12d530de354e14a53ac17cd923328b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1ce0ce09aac4066bbd63b707a0cb3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e2fb27a84244814896ebf22176404a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69cdb72771bb466ba7b35ca550df1ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}